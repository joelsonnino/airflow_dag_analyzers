[
  {
    "dag_id": "Buff_Play_from_Postgres_to_Snowflake",
    "summary": "This DAG extracts data from a PostgreSQL database, loads it into Snowflake, and refreshes external tables in Snowflake.",
    "problems": [
      "Missing retry policy for connections to PostgreSQL and Snowflake",
      "Hardcoded connection strings and paths",
      "Lack of logging or monitoring",
      "Potential for deadlocks or other concurrency issues",
      "Use of deprecated `awswrangler` library"
    ],
    "risk_level": "MEDIUM",
    "suggestion": "Implement a retry policy for connections, use environment variables for connection strings and paths, add logging and monitoring, and consider using the latest version of `airflow` and `awswrangler` libraries.",
    "code_fix": null,
    "filename": "Buff Play Mobile.py"
  },
  {
    "dag_id": "Challenge_contest_from_Postgres_to_Snowflake",
    "summary": "DAG to transfer data from PostgreSQL to Snowflake using Airflow. It loads and refreshes external tables in Snowflake.",
    "problems": [
      "Missing retries for PythonOperator tasks",
      "Hardcoded paths for S3 objects",
      "No validation for connection string",
      "Potential SQL injection vulnerability in query building",
      "Lack of logging mechanisms"
    ],
    "risk_level": "MEDIUM",
    "suggestion": "Consider adding retries and logging mechanisms to improve robustness. Use dynamic path generation instead of hardcoded paths.",
    "code_fix": "",
    "filename": "Challenge Contests.py"
  },
  {
    "dag_id": "Challenge_dictionary_from_Postgres_to_Snowflake",
    "summary": "This DAG loads data from PostgreSQL to Snowflake, refreshes external tables in Snowflake, and disposes of the PostgreSQL engine.",
    "problems": [
      "Missing retries for connections and operations may lead to transient errors being caught as fatal errors.",
      "Hardcoded paths and query strings make the code less flexible and maintainable. Consider using environment variables or a configuration file instead.",
      "Insufficient error handling in some functions (e.g., `load_table`), which could mask critical issues.",
      "Deprecation of SQLAlchemy's connection string parsing functionality with `parse.quote_plus`. Prefer using `sqlalchemy` library's built-in support for connection strings.",
      "Lack of logging or monitoring mechanisms to track the DAG's execution and identify potential problems."
    ],
    "risk_level": "MEDIUM",
    "suggestion": "Implement retries, use a more flexible configuration system, improve error handling, update deprecated functionality, and add logging/monitoring mechanisms.",
    "code_fix": null,
    "filename": "Challenges_dictionary.py"
  },
  {
    "dag_id": "Complex_google_ads_snowflake_uploader_dag",
    "summary": "This DAG uploads Google Ads data to Snowflake using a custom operator.",
    "problems": [
      "Missing retries for tasks (default retry count is not specified)",
      "Hardcoded path in `init_conf()` function",
      "Potential data loss due to missing `catchup` value (set to True by default)",
      "Deprecation notice for `empty` operator in Airflow 2.x",
      "Lack of docstrings for functions"
    ],
    "risk_level": "MEDIUM",
    "suggestion": "Consider adding retries for tasks, updating hardcoded paths, and enabling catchup for all tasks. Add docstrings to improve readability.",
    "code_fix": "Update `retries` parameter in `init_args()` function, update hardcoded path in `init_conf()` function, add `catchup=False` to all task definitions, and add docstrings to functions",
    "filename": "Complex_google_ads_snowflake_uploader.py"
  },
  {
    "dag_id": "test_recover_everflow",
    "summary": "This DAG retrieves data from a PostgreSQL database and then fires events to an external endpoint based on the retrieved data. The data is used to update a table with gaming points, matches, and other relevant information.",
    "problems": [
      "Missing retries in the `fireEvents` task",
      "Hardcoded paths and URLs",
      "Lack of logging and error handling",
      "Use of deprecated `airflow.providers.snowflake.hooks.snowflake` hook",
      "No docstrings or comments for functions and tasks"
    ],
    "risk_level": "MEDIUM",
    "suggestion": "Add retries to the `fireEvents` task, use environment variables or a configuration file for hardcoded paths and URLs, add logging and error handling, update the hook to a non-deprecated version, and add docstrings and comments to functions and tasks.",
    "code_fix": null,
    "filename": "EFtestDag.py"
  },
  {
    "dag_id": "Leaderboard_contest_from_Postgres_to_Snowflake",
    "summary": "This DAG loads leaderboard data from PostgreSQL and Snowflake, then refreshes external tables in Snowflake.",
    "problems": [
      "Missing retry policy for connections",
      "No validation of `cfg[t][2]` value for 'none' cases",
      "Potential data loss due to missing `offset` parameter in Snowflake queries",
      "No error handling for PythonOperator failures"
    ],
    "risk_level": "MEDIUM",
    "suggestion": "Add retry policy for connections, validate `cfg[t][2]` values, add offset parameter and implement proper error handling for PythonOperator failures. Also consider adding docstrings to functions.",
    "code_fix": "add retry policy: `arguments['retries'] = 3` or use `retry_limit` parameter in SnowflakeHook; add validation: `if cfg[t][2] == 'none': ... else: ...`; add offset: `limit {chunkSize} OFFSET {offset}`; implement error handling: `try-except block for PythonOperator failures`",
    "filename": "Leaderboard Contests.py"
  },
  {
    "dag_id": "melee_postback_dag",
    "summary": "This DAG sends Google Ads events to Airflow and prints the response.",
    "problems": [
      "Missing retry policy for failed requests",
      "Hardcoded access token in clear text",
      "Missing docstring for the PythonOperator"
    ],
    "risk_level": "MEDIUM",
    "suggestion": "Implement retries, secure sensitive data, and add a docstring to the `send_events` function.",
    "code_fix": null,
    "filename": "Melee_events.py"
  },
  {
    "dag_id": "upload_OW_tables_to_s3",
    "summary": "Uploads full tables from PostgreSQL to S3 and refreshes external Snowflake tables.",
    "problems": [
      "missing retries in case of PostgreSQL connection errors or Snowflake query failures",
      " hardcoded path to S3 bucket, consider using Airflow's built-in storage options",
      "potential data type issues due to missing explicit casting in `wr.s3.to_parquet` method"
    ],
    "risk_level": "MEDIUM",
    "suggestion": "Consider adding retries for PostgreSQL connections and Snowflake queries to handle potential failures.",
    "code_fix": null,
    "filename": "OW_tables_to_S3.py"
  },
  {
    "dag_id": "Desktop_Tracking_MongoDB",
    "summary": "This DAG exports MongoDB data to S3 as Parquet files based on predefined aggregation pipeline.",
    "problems": [
      "Missing retries for tasks with failed exceptions",
      "Hardcoded paths and folder names",
      "No error handling or logging in the `export_mongodb_to_s3` function",
      "Deprecation of `DummyOperator` in favor of `NoneTask`"
    ],
    "risk_level": "MEDIUM",
    "suggestion": "Implement retries for tasks with failed exceptions, use environment variables for paths and folder names, add error handling and logging to the `export_mongodb_to_s3` function, and update `DummyOperator` to `NoneTask`",
    "code_fix": null,
    "filename": "Pings_from_Mongo.py"
  },
  {
    "dag_id": "Desktop_Tracking_MongoDB_v2",
    "summary": "Export MongoDB aggregation results to S3 as Parquet files.",
    "problems": [
      "Missing retries in case of failed connections or data processing errors",
      "Hardcoded path for storing Parquet files on S3 without considering environment variables or cloud provider's bucket naming conventions",
      "Lack of logging and monitoring mechanism for tracking DAG run success/failure, timeouts, and resource utilization"
    ],
    "risk_level": "MEDIUM",
    "suggestion": "Consider adding retries with exponential backoff to handle transient errors; use AWS SDK's built-in retry mechanism or a third-party library like `tenacity` for better error handling; add logging and monitoring using Airflow's built-in logging and `prometheus` integration for cloud providers.",
    "code_fix": null,
    "filename": "Pings_from_Mongo2.py"
  },
  {
    "dag_id": "Desktop_Tracking_MongoDB_limit_for_test_v2",
    "summary": "Exports desktop tracking aggregation results from MongoDB to S3 as Parquet files.",
    "problems": [
      "Missing retry policy for failed tasks",
      "Hardcoded MongoDB connection settings",
      "Inconsistent data cleaning and formatting steps",
      "Lack of error handling in Python callable 'export_mongodb_to_s3'",
      "Uncommented `schedule_interval` to enable task execution"
    ],
    "risk_level": "MEDIUM",
    "suggestion": [
      "Add retry policy with increasing delays for failed tasks",
      "Use environment variables or config files to store sensitive connection settings",
      "Consolidate and standardize data cleaning and formatting steps",
      "Implement error handling in Python callable 'export_mongodb_to_s3'",
      "Enable task execution by uncommenting `schedule_interval`"
    ],
    "code_fix": null,
    "filename": "Pings_from_Mongo3.py"
  },
  {
    "dag_id": "Desktop_Tracking_MongoDB_v4",
    "summary": "This DAG exports MongoDB data to S3 as Parquet files, filtered by date range.",
    "problems": [
      "Missing retry policy for tasks with failures",
      "Hardcoded path in `wr.s3.to_parquet` call",
      "No handling for potential MongoDB connection errors",
      "Lack of logging or monitoring for DAG execution"
    ],
    "risk_level": "MEDIUM",
    "suggestion": "Implement retry policies for task failures, use environment variables for S3 paths, and add logging/monitoring to ensure DAG reliability.",
    "code_fix": "Add `retries` parameter to `PythonOperator` calls and set a default value for `retry_delay`; update `wr.s3.to_parquet` call to use an environment variable for the path; consider adding try-except blocks for potential MongoDB errors",
    "filename": "Pings_from_Mongo4.py"
  },
  {
    "dag_id": "Desktop_Tracking_MongoDB_v5",
    "summary": "This DAG exports MongoDB data to S3 in batches, processing each batch and then storing the results in a Parquet file.",
    "problems": [
      "Missing retries for tasks",
      "Hardcoded path in s3.to_parquet() function",
      "Potential out of order documents are not handled properly",
      "Lack of documentation and comments in the code",
      "Use of deprecated operator 'DummyOperator' instead of 'StartOperator'"
    ],
    "risk_level": "MEDIUM",
    "suggestion": "Add retries for tasks, refactor path to be a variable or parameter, handle out of order documents properly, add docstrings and comments to explain the code, and update operators to start with 'StartOperator'",
    "code_fix": null,
    "filename": "Pings_from_Mongo5.py"
  },
  {
    "dag_id": "recover_buff_transactions_dag",
    "summary": "This DAG recovers buff transactions from a PostgreSQL database, transforms the data into a format suitable for Snowflake, and refreshes an external table in Snowflake.",
    "problems": [
      "No retries in case of failed PythonOperator execution",
      "Hardcoded path to S3 bucket",
      "Potential performance issue due to large chunk size (75,000)",
      "Lack of error handling in Snowflake refresh operation"
    ],
    "risk_level": "MEDIUM",
    "suggestion": [
      "Implement retries for failed PythonOperator execution",
      "Use environment variables or a secure storage mechanism to store S3 bucket path",
      "Consider using a more efficient chunk size or parallel processing",
      "Add try-except block for Snowflake refresh operation and handle potential errors"
    ],
    "code_fix": "add retry logic to the recover_buff_transactions function and consider using an environment variable for the S3 bucket path, and use a more efficient chunk size or parallel processing approach.",
    "filename": "buff_transactions_recovery_dag.py"
  },
  {
    "dag_id": "buff_transactions_to_s3_incremental_cp",
    "summary": "This DAG loads buff transactions data from PostgreSQL to S3, and then refreshes an external table in Snowflake.",
    "problems": [
      "The `retries` parameter is set to 2, which might not be enough for all possible failures. Consider increasing it or implementing a more robust retry strategy.",
      "Hardcoded paths are used in the code, which can make it difficult to maintain and update the DAG. Consider using environment variables or a config file instead.",
      "The `catchup` parameter is set to False, which means that the DAG will only run according to the schedule interval. Consider setting it to True to allow for catch-up runs.",
      "The `schedule_interval` is set to None, which means that the DAG will not run at all. Consider setting a valid schedule interval or using Airflow's built-in scheduling features.",
      "The `concurrency` parameter is set to 1, which means that only one task can run at a time. Consider increasing it to allow for more concurrent tasks."
    ],
    "risk_level": "MEDIUM",
    "suggestion": "Review the retry strategy and adjust the number of retries as needed. Update hardcoded paths to use environment variables or a config file. Set `catchup` to True and a valid schedule interval to allow for catch-up runs and scheduling.",
    "code_fix": null,
    "filename": "buff_transactions_to_s3_daily_by_chunks_parquet.py"
  },
  {
    "dag_id": "clean_gateway_events_dag",
    "summary": "DAG that deletes old gateway events from an S3 bucket every day at 8am",
    "problems": [
      "Missing docstrings for the DAG and its tasks",
      "Hardcoded path in the `clean_gateway` function",
      "Potential issue with `retries` set to 2, but no backoff strategy specified",
      "No logging or monitoring mechanism implemented"
    ],
    "risk_level": "MEDIUM",
    "suggestion": "Add docstrings to the DAG and its tasks, implement a more robust error handling mechanism, and consider using a more sophisticated backoff strategy for retries.",
    "code_fix": null,
    "filename": "cleanGateway.py"
  },
  {
    "dag_id": "contests_db_extraction_dag",
    "summary": "Extracts data from various databases and refreshes external tables.",
    "problems": [
      "Missing retries for the extract_data function in case of database connection errors.",
      "Hardcoded paths and table names may make the DAG less flexible and harder to maintain.",
      "No error handling for the refresh_ext_tables function.",
      "Using a hardcoded schedule interval (15 4 * * *) which might not be suitable for all environments.",
      "Lack of docstrings for functions and operators."
    ],
    "risk_level": "MEDIUM",
    "suggestion": "Add retries to the extract_data function, use parameterized paths and table names, add error handling to refresh_ext_tables, and consider using a more flexible schedule interval or a cron expression.",
    "code_fix": null,
    "filename": "contests_DB_extract.py"
  },
  {
    "dag_id": "delayed_dwh_etl_dag",
    "summary": "This DAG loads data from Snowflake into Airflow's DWH and truncates the external table.",
    "problems": [
      "Missing retries for PythonOperator tasks",
      "Hardcoded path in `SnowflakeHook` constructor",
      "Lack of docstrings for DAG and operators"
    ],
    "risk_level": "MEDIUM",
    "suggestion": "Add retries to PythonOperator tasks, avoid hardcoded paths by using environment variables or a more flexible connection handler, and add docstrings to the DAG and operators.",
    "code_fix": "import os; ... shk = SnowflakeHook(snowflake_conn_id=os.environ['SNOWFLAKE_CONN_ID'])",
    "filename": "delayed_etl_tasks.py"
  },
  {
    "dag_id": "dwh_etl1",
    "filename": "dwh_etl.py"
  },
  {
    "dag_id": "fire_incentives_to_everflow",
    "summary": "This DAG processes Everflow incentives data, merging old and new data, and triggering events for each incentive.",
    "problems": [
      "Missing retries in the `fireEvents` function",
      "Hardcoded paths (e.g., `s3://tperson-bucket/tperson-bucket/Everflow_incentives/main.parquet`)",
      "Deprecated operator: `airflow.providers.snowflake.hooks.snowflake.SnowflakeHook`",
      "Lack of docstrings for the DAG and its tasks",
      "Improper scheduling (every 2 hours, but no clear reason)",
      "No error handling in the `fireEvents` function"
    ],
    "risk_level": "MEDIUM",
    "suggestion": "Add retries to the `fireEvents` function, use absolute paths or environment variables for hardcoded paths, update the DAG to use a more modern and robust Snowflake hook, add docstrings to the DAG and its tasks, review the scheduling interval and consider using a more flexible approach, and improve error handling in the `fireEvents` function",
    "code_fix": null,
    "filename": "everflow_postback_dag.py"
  },
  {
    "dag_id": "MongoDB_export_to_s3",
    "summary": "This DAG exports data from MongoDB to S3 using the `awswrangler` library.",
    "problems": [
      "Missing retry policy for the `export_mongodb_to_s3` task",
      "Hardcoded connection string and password for the MongoDB connection",
      "Potential issue with concurrency control, as only one task can run at a time",
      "No error handling or logging mechanisms in place"
    ],
    "risk_level": "MEDIUM",
    "suggestion": "Implement retry policy, secure connection string and password, use Airflow's built-in concurrency control, and add error handling and logging mechanisms.",
    "code_fix": null,
    "filename": "export_MongoDB_to_s3.py"
  },
  {
    "dag_id": "google_ads_upload_data_dag",
    "summary": "This DAG uploads data from Snowflake to Google Ads using Airflow. It retrieves data from various Snowflake tables and stores it in S3 CSV files.",
    "problems": [
      "Missing retries for `wr.s3.to_csv` operation",
      "Hardcoded path in `wr.s3.to_csv` operation",
      "Lack of error handling for `cur.execute` query execution",
      "Potential issue with `cur.rowcount == 0:` check"
    ],
    "risk_level": "MEDIUM",
    "suggestion": "Consider adding retries to the `wr.s3.to_csv` operation and proper error handling for query execution. Also, consider using a more dynamic path generation.",
    "code_fix": "Add retry mechanism for `wr.s3.to_csv` like so: `for attempt in range(3): try: wr.s3.to_csv(df=table, path=path); break except Exception as e: ...`",
    "filename": "google_ads_upload_data.py"
  },
  {
    "dag_id": "Googlesheets_snowflake_uploader_dag",
    "summary": "This DAG is used to upload data from Google Sheets to Snowflake, leveraging a custom operator. It uses Airflow's Variable feature for configuration and has retry capabilities.",
    "problems": [
      "Missing retry policy: The 'retries' argument in the default_args dictionary does not specify a maximum number of retries if the task fails.",
      "Hardcoded path: The `google_conn_id`, `snowflake_conn_id`, and other connection IDs are hardcoded. It's better to store them as variables or environment variables.",
      "Deprecated operator usage: Although the custom operator is used, it's recommended to use Airflow's built-in operators (e.g., `GoogleSheetsToSnowflakeOperator` should be renamed) for compatibility.",
      "Improper scheduling: The schedule_interval is set to None, which might not be what the author intended. Consider using a specific cron expression or a callback function to trigger the DAG.",
      "Lack of error handling: There's no explicit try-except block to handle potential exceptions during task execution."
    ],
    "risk_level": "MEDIUM",
    "suggestion": "Consider implementing retries with exponential backoff, defining environment variables for connection IDs, and refactoring the custom operator usage. Additionally, add a try-except block around the `gen_pipeline_tasks` function call to handle potential exceptions.",
    "code_fix": null,
    "filename": "googlesheet_reader.py"
  },
  {
    "dag_id": "recover_inc_table_dag",
    "summary": "This DAG recovers data from various tables in different databases and refreshes external tables in Snowflake.",
    "problems": [
      "Missing retry logic for database connections",
      "Hardcoded port number in the connection string",
      "Potential performance issue due to excessive loop iterations (3000)",
      "No error handling for exceptions during table creation"
    ],
    "risk_level": "MEDIUM",
    "suggestion": "Implement retry logic, validate connection strings, optimize loop iterations, and add error handling.",
    "code_fix": "Add retry logic using `airflow.providers.postgres.hooks.PostgresHook` instead of hardcoded port number and adjust the while loop condition to avoid excessive iterations.",
    "filename": "inc_tables_recovery_dag.py"
  },
  {
    "dag_id": "payment_service_connection_test_dag",
    "summary": "DAG tests a PostgreSQL connection and runs a SQL query to read from a 'products' table.",
    "problems": [
      "Missing retries for the task if it fails",
      "Hardcoded path in the `connection_string` variable",
      "Lack of logging and error handling mechanisms",
      "No validation for empty or invalid connection parameters"
    ],
    "risk_level": "MEDIUM",
    "suggestion": "Consider adding more retries, validating connection parameters, and using logging to track errors.",
    "code_fix": null,
    "filename": "payment_service_test.py"
  },
  {
    "dag_id": "postgres_dbs_extraction_dag",
    "summary": "This DAG extracts data from various PostgreSQL databases, processes it, and stores the results in S3.",
    "problems": [
      "Missing retry policy for some tasks",
      "Hardcoded connection strings and database schema names",
      "Lack of error handling in the extract_data function",
      "Using deprecated `airflow.operators.python_operator` instead of `airflow.operators.python`",
      "Insufficient logging"
    ],
    "risk_level": "MEDIUM",
    "suggestion": "Improve retry policy, refactor connection strings and database schema names, add error handling, switch to deprecated operator, and enhance logging.",
    "code_fix": null,
    "filename": "postgres_dbs_extraction.py"
  },
  {
    "dag_id": "Account_Session_from_Postgres_to_Snowflake",
    "summary": "This DAG extracts data from Postgres, processes it and loads the data to Snowflake.",
    "problems": [
      "Missing retries in the connection to Postgres",
      "No validation of database schema changes",
      "Hardcoded path for loading data to S3",
      "Lack of error handling in the refresh_ext_tables function",
      "Potential performance issue due to concurrent execution"
    ],
    "risk_level": "MEDIUM",
    "suggestion": "Add retries to the connection to Postgres, validate database schema changes and handle potential errors in the refresh_ext_tables function. Consider using a more robust method for handling concurrent execution.",
    "code_fix": "```.python\n# add retries to connection\ntry:\n    engine.connect()\nexcept SQLAlchemyError as err:\n    print('connection error')\n    print('error', err.__cause__)\n    # retry after 5 minutes\n    time.sleep(300)\n    engine.connect()\n```\n```",
    "filename": "session_service.py"
  },
  {
    "dag_id": "uar",
    "summary": "This DAG processes UAR data by loading tables from a Snowflake database and enriching them with additional information.",
    "problems": [
      "Missing retry policy for tasks that may fail during execution (only retries are configured for the entire DAG)",
      "Hardcoded paths in `view_to_table_full` function",
      "Lack of error handling in `view_to_table_full` function",
      "Deprecation notice for `imod` from `operator` module"
    ],
    "risk_level": "MEDIUM",
    "suggestion": "Consider adding retry policies to individual tasks, handling potential errors in the `view_to_table_full` function, and updating the `imod` operator to its latest version.",
    "code_fix": "Replace `imod` with `operator.imod` and add try-except blocks around `cur.execute(trun)` and `cur.execute(insr)` calls",
    "filename": "uar.py"
  },
  {
    "dag_id": "analytic_service_tables_extract_dag",
    "summary": "Extracts data from Snowflake tables and refreshes external tables.",
    "problems": [
      "Missing retries for PythonOperator: `python_callable=extract_tables`",
      "Hardcoded paths in `extract_tables` function (e.g. `path=f`s3://tperson-bucket/tperson-bucket/airflow_replica/{t}/part_{part}.parquet)",
      "Lack of logging or error handling in `extract_tables` function"
    ],
    "risk_level": "MEDIUM",
    "suggestion": "Add retries to PythonOperator, use environment variables for hardcoded paths, and add logging or error handling to `extract_tables` function.",
    "code_fix": "python_callable=extract_tables with retry_delay and max_retries parameters; use AWS config file for hardcoded paths; add logging statements in extract_tables function",
    "filename": "upload_analytic_service_tables_to_s3.py"
  },
  {
    "dag_id": "upload_pg_tables_to_s3_v2",
    "summary": "This DAG loads PostgreSQL tables into S3 Parquet files with retries, schedules every 45 minutes.",
    "problems": [
      "The DAG uses a fixed chunk size of 500000 rows, which might not be optimal for all tables. Consider using a more adaptive chunk size based on the table's row count.",
      "The `refresh_ext_tables` function uses hardcoded Snowflake connection details. Consider using Airflow's built-in hook to connect to Snowflake instead.",
      "The DAG does not handle database errors properly. It catches SQLAlchemyError but does not re-raise it or retry the operation. Consider adding retries or handling exceptions more robustly."
    ],
    "risk_level": "MEDIUM",
    "suggestion": "Optimize chunk size, use Airflow's built-in Snowflake hook, and improve error handling.",
    "code_fix": "Replace `chunkSize=500000` with `chunkSize = min(1000000, table.shape[0])` to adapt the chunk size based on the table's row count.",
    "filename": "upload_pg_tables_to_s3_v2.py"
  }
]
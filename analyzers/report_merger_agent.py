#!/usr/bin/env python3
"""
Intelligent DAG Report Merger & Standalone HTML Generator

This module serves as the central orchestration point. It takes the individual
analysis reports (code audit, statistics, log analysis), merges them,
sends the aggregated data to an AI language model for a holistic final analysis,
and then prepares this comprehensive data for the Streamlit dashboard or
generates a self-contained, portable HTML dashboard file.
"""

import os
import json
from pathlib import Path
import requests
from collections import defaultdict
from typing import Dict, List, Any, Optional
from datetime import datetime

# --- Configuration ---
# Root directory of the project
PROJECT_ROOT = Path(__file__).resolve().parent.parent
# Directory where all intermediate and final reports are stored
REPORTS_DIR = PROJECT_ROOT / "reports"
# Path to the HTML template used for generating standalone dashboards
# (Note: This template needs to exist in the project root for standalone HTML generation)
TEMPLATE_HTML_PATH = PROJECT_ROOT / "dashboard_template.html"

# Directory where timestamped daily standalone HTML dashboards will be saved
DAILY_REPORTS_OUTPUT_DIR = REPORTS_DIR / "daily_dashboards"

# NUOVO: Path for the pre-generated dashboard data JSON file for Streamlit
DASHBOARD_DATA_OUTPUT_PATH = REPORTS_DIR / "final_dashboard_data.json" # <--- AGGIUNTA

def read_file(file_path: Path) -> str:
    """
    A helper function to safely read the content of a file.
    Handles FileNotFoundError and other exceptions gracefully.
    """
    try:
        return file_path.read_text(encoding='utf-8')
    except FileNotFoundError:
        print(f"‚ö†Ô∏è Report file not found at {file_path}. Returning empty string.")
        return ""
    except Exception as e:
        print(f"‚ùå Error reading {file_path}: {e}")
        return ""

class OllamaClient:
    """
    Client for interacting with the Ollama API to leverage local AI models.
    Provides methods to send prompts and receive structured JSON responses.
    """
    def __init__(self, model: str = "llama3.2", host: Optional[str] = None):
        # Determine Ollama host: prioritize environment variable, then default to localhost
        if host is None:
            host = os.environ.get("OLLAMA_HOST", "http://localhost:11434")
        self.model = model
        self.host = host
        print(f"ü§ñ Ollama client configured for model '{self.model}' at {self.host}")

    def ask(self, prompt: str, system_prompt: str) -> Dict:
        """
        Sends a prompt to the Ollama API and attempts to parse the response as JSON.
        Includes robust error handling for network issues and JSON parsing failures.
        """
        try:
            response = requests.post(
                f"{self.host}/api/generate",
                json={
                    "model": self.model,
                    "prompt": prompt,
                    "system": system_prompt,
                    "stream": False,  # Request a single, complete response
                    "format": "json", # Instruct Ollama to try and return JSON
                    "options": {"temperature": 0.2} # Lower temperature for more deterministic output
                },
                timeout=180 # Set a generous timeout for AI response
            )
            response.raise_for_status() # Raise HTTPError for bad responses (4xx or 5xx)
            response_data = response.json()
            json_response_string = response_data.get("response", "{}")
            return json.loads(json_response_string) # Parse the AI's response string as JSON
        except requests.exceptions.RequestException as e:
            print(f"‚ùå Connection error with Ollama: {e}")
            return {"error": "Connection to Ollama failed.", "details": str(e)}
        except json.JSONDecodeError as e:
            # Handle cases where Ollama might not return perfect JSON
            raw_text = response.json().get("response", "") if 'response' in locals() else "Raw response not available."
            print(f"‚ùå Failed to parse JSON from Ollama: {e}\n   Raw response: {raw_text[:300]}...")
            return {"error": "Failed to parse AI JSON response.", "raw_text": raw_text}
        except Exception as e:
            print(f"‚ùå An unexpected error occurred in Ollama client: {e}")
            return {"error": "An unexpected error occurred.", "details": str(e)}

class ReportParser:
    """
    Handles parsing of various JSON report files generated by individual analyzers.
    Provides methods to read and structure data from stats, code audit, and log analysis reports.
    """
    def _read_json(self, file_path: Path) -> Optional[Any]:
        """Internal helper to read and parse a JSON file, returning None on error."""
        try:
            content = read_file(file_path)
            if not content: return None
            return json.loads(content)
        except json.JSONDecodeError as e:
            print(f"‚ùå Error parsing JSON from {file_path}: {e}")
            return None
        except Exception as e:
            print(f"‚ùå Error reading {file_path}: {e}")
            return None

    def parse_stats_report(self, file_path: Path) -> Dict[str, Dict[str, Any]]:
        """Parses the DAG statistics report."""
        stats_data = self._read_json(file_path)
        if not isinstance(stats_data, dict): # Ensure it's a dict (expected format)
            print(f"‚úÖ Parsed 0 DAGs from stats report (file empty or invalid).")
            return {}
        print(f"‚úÖ Parsed {len(stats_data)} DAGs from the stats report.")
        return stats_data

    def parse_audit_report(self, file_path: Path) -> Dict[str, Dict[str, Any]]:
        """Parses the AI code audit report."""
        audit_list = self._read_json(file_path)
        if not isinstance(audit_list, list): # Ensure it's a list (expected format)
            print(f"‚úÖ Parsed 0 DAGs from audit report (file empty or invalid).")
            return {}
        # Convert list of audit items to a dictionary keyed by dag_id for easy lookup
        audit_data = {item['dag_id']: item for item in audit_list if item.get('dag_id')}
        print(f"‚úÖ Parsed {len(audit_data)} DAGs from audit report.")
        return audit_data

    def parse_analysis_report(self, file_path: Path) -> Dict[str, List[Dict[str, Any]]]:
        """Parses the AI error log analysis report."""
        analysis_list = self._read_json(file_path)
        if not isinstance(analysis_list, list): # Ensure it's a list (expected format)
            print(f"‚úÖ Parsed 0 log analyses (file empty or invalid).")
            return {}
        # Aggregate log error analyses by DAG ID using defaultdict
        analysis_data = defaultdict(list)
        for item in analysis_list:
            dag_name = item.get('error', {}).get('dag_name')
            if dag_name:
                analysis_data[dag_name].append(item)
        print(f"‚úÖ Parsed log analyses for {len(analysis_data)} DAGs.")
        return dict(analysis_data)

class IntelligentReportMerger:
    """
    Core class responsible for merging all individual reports and performing
    a final, holistic AI analysis to generate the comprehensive dashboard data.
    It can either provide data for Streamlit or generate a standalone HTML report.
    """
    def __init__(self):
        self.client = OllamaClient()
        self.parser = ReportParser()

    def _collect_and_merge_data(self) -> Dict[str, Dict]:
        """
        Reads data from all individual JSON reports (stats, code audit, log errors)
        and merges them into a single dictionary, keyed by DAG ID.
        """
        base_data = self.parser.parse_stats_report(REPORTS_DIR / "dag_stats.json")
        audit_data = self.parser.parse_audit_report(REPORTS_DIR / "dag_ai_audit.json")
        analysis_data = self.parser.parse_analysis_report(REPORTS_DIR / "log_analysis.json")
        
        # Use defaultdict to ensure all DAGs have necessary keys, even if empty
        dag_data = defaultdict(lambda: {"stats": {}, "code_audit": {}, "log_errors": []})
        # Collect all unique DAG IDs found across all reports
        all_dags = set(base_data.keys()) | set(audit_data.keys()) | set(analysis_data.keys())

        # Populate the merged dictionary
        for dag_id in all_dags:
            if dag_id in base_data: dag_data[dag_id]['stats'] = base_data[dag_id]
            if dag_id in audit_data: dag_data[dag_id]['code_audit'] = audit_data[dag_id]
            if dag_id in analysis_data: dag_data[dag_id]['log_errors'].extend(analysis_data[dag_id])
        
        return dict(dag_data) # Convert defaultdict back to a regular dict

    def _run_ai_final_analysis(self, all_dag_data: Dict) -> Dict:
        """
        Iterates through each DAG's merged data, constructs a comprehensive prompt,
        and sends it to the Ollama client for a final, holistic AI analysis.
        This analysis generates the executive summary, health score, priority, and key recommendations.
        """
        analyzed_dags = {}
        for dag_id, data in all_dag_data.items():
            if dag_id == "unknown": continue # Skip DAGs with unknown IDs
            print(f"  -> AI Analyzing: {dag_id}")
            
            # Prepare a summary of the raw data to send to the AI
            data_summary = {
                "performance_stats": data.get("stats"),
                "code_audit": data.get("code_audit"),
                # Truncate error lines for prompt efficiency
                "recent_log_errors": [{"severity": e["severity"], "error": e["error"]["error_line"][:200]} for e in data.get("log_errors", [])]
            }
            
            # Construct the detailed prompt for the AI model
            prompt = f"""
            Analyze the comprehensive data for the Airflow DAG '{dag_id}'.
            Data: {json.dumps(data_summary, indent=2)}
            Provide a final analysis as a valid JSON object with these keys:
            - "executive_summary": A 2-3 sentence overview of the DAG's health, problems, and state.
            - "health_score": An integer score from 0 (broken) to 100 (perfect). Consider success rate, failure count, code risk.
            - "priority": Priority for attention: "CRITICAL", "HIGH", "MEDIUM", or "LOW".
            - "key_recommendations": A list of 2-3 specific, actionable recommendations.
            """
            system_prompt = "You are a Senior Airflow Operations Engineer. Provide a concise, actionable JSON summary for a DAG based on its performance stats, code quality, and runtime errors. Return a valid JSON object only."
            
            ai_result = self.client.ask(prompt, system_prompt)
            # Store both raw data and the AI's analysis
            analyzed_dags[dag_id] = {"raw_data": data, "ai_analysis": ai_result}
        return analyzed_dags

    # NUOVO: Helper per salvare i dati della dashboard
    def _save_dashboard_data(self, data: Dict[str, Dict]):
        """Helper to save the final dashboard data to a JSON file."""
        DASHBOARD_DATA_OUTPUT_PATH.parent.mkdir(parents=True, exist_ok=True)
        with open(DASHBOARD_DATA_OUTPUT_PATH, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2)
        print(f"üíæ Dati della dashboard salvati in: {DASHBOARD_DATA_OUTPUT_PATH}")

    def generate_dashboard_data(self) -> Dict[str, Dict]:
        """
        Public method to generate the fully analyzed and merged DAG data
        specifically for consumption by the Streamlit dashboard.
        This method orchestrates data collection and final AI analysis.
        """
        print("üöÄ Starting intelligent DAG data generation for dashboard...")
        
        # Step 1: Collect and merge raw data from all individual reports
        dag_data = self._collect_and_merge_data()
        if not dag_data:
            print("‚ùå No DAG data found. Halting.")
            return {}

        # Step 2: Run the final AI analysis on the merged data
        print(f"üìä Processing {len(dag_data)} DAGs for final AI analysis...")
        analyzed_dags = self._run_ai_final_analysis(dag_data)
        
        # NUOVO: Salva i dati prima di retornarli
        self._save_dashboard_data(analyzed_dags)
        
        print("‚úÖ Dashboard data generated successfully.")
        return analyzed_dags

    def generate_standalone_dashboard(self):
        """
        Generates a self-contained HTML file that includes all the AI-analyzed
        DAG data embedded within it. This file can be easily shared and viewed
        in any web browser without needing a running Streamlit server.
        """
        print("üöÄ Starting intelligent DAG report generation for standalone HTML...")
        
        dag_data = self._collect_and_merge_data()
        if not dag_data:
            print("‚ùå No DAG data found. Halting.")
            return

        print(f"üìä Processing {len(dag_data)} DAGs for final AI analysis...")
        analyzed_dags = self._run_ai_final_analysis(dag_data)

        # NUOVO: Salva i dati della dashboard anche qui per coerenza
        self._save_dashboard_data(analyzed_dags) # <--- AGGIUNTA

        try:
            template_content = TEMPLATE_HTML_PATH.read_text(encoding='utf-8')
        except FileNotFoundError:
            print(f"‚ùå ERROR: Template file not found at {TEMPLATE_HTML_PATH}")
            print("Please ensure 'dashboard_template.html' exists in the project root.")
            return

        # Convert the entire analyzed data dictionary to a JSON string
        data_as_json_string = json.dumps(analyzed_dags, indent=2)

        # Inject the JSON data into the HTML template at the specified placeholder
        final_html = template_content.replace(
            "//__DASHBOARD_DATA_PLACEHOLDER__", data_as_json_string
        )

        # Ensure the output directory for daily dashboards exists
        DAILY_REPORTS_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

        # Create a unique filename with the current date for archiving
        timestamp = datetime.now().strftime("%Y-%m-%d")
        final_filename = f"dashboard_{timestamp}.html"
        final_path = DAILY_REPORTS_OUTPUT_DIR / final_filename

        # Save the final, self-contained HTML file
        final_path.write_text(final_html, encoding='utf-8')
        
        print("="*50)
        print(f"‚úÖ Success! Standalone dashboard created at: {final_path}")
        print("You can now find this file in the 'reports/daily_dashboards/' folder.")
        print("="*50)

if __name__ == "__main__":
    # When run directly, this script defaults to generating the standalone HTML dashboard.
    # The Streamlit dashboard invokes generate_dashboard_data() directly.
    merger = IntelligentReportMerger()
    merger.generate_standalone_dashboard()